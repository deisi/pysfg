{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "\n",
    "import pandas as pd\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pysfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quartz or Gold Spectrum for Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idear of the analysis is to seperate some distinkt properties of the data analysis. the `static_spectra.py` script contains the typical logic of what one want to do with static spectral data. For each usecase, a new `some_name.yaml` file needs to be passed to the `static_spectra.py` file, to configure a specific set of data. What it basically does is:\n",
    "\n",
    "- read of data files\n",
    "- calibrate pixels to wavenumbers \n",
    "- trim data (pixels and spectrum index)\n",
    "- take median of scans and pp_delays\n",
    "- save result in a structured `.json` file\n",
    "\n",
    "The syntax for using `static_spectra.py` is:\n",
    "\n",
    "```\n",
    "python static_spectra.py path_to_config.yaml\n",
    "```\n",
    "\n",
    "or from within the notebook:\n",
    "```\n",
    "%run static_spectra.py path_to_config.yaml\n",
    "```\n",
    "\n",
    "\n",
    "`yaml` files are a structured way of storing information or in this case configuration. [Here](https://rollout.io/blog/yaml-tutorial-everything-you-need-get-started/) is a possible tutorial explaining many aspects of yaml. Understanding yaml is not very difficult. If you are confused by `simple_gold.yaml` take a peak at the mentioned tutorial. In short, indentation matters, `:` seperates `key:value` just like python dicts, lists can be given with`[1, 2, 3]`  or\n",
    "\n",
    "``` yaml\n",
    "- 1\n",
    "- 2\n",
    "- 3 \n",
    "```\n",
    "\n",
    "numbers are read as `float` or `int`, text is read as `string`\n",
    "\n",
    "Lets go through the structure of `simple_gold.yaml` for now:\n",
    "\n",
    "``` yaml\n",
    "calibration:\n",
    "    vis_wl: 799.7 # Wavelength of the visible\n",
    "    calib_central_wl: 680 # central wavelength during calibration\n",
    "    calib_coeff: [0.080881, 615.18] # calibration coefficients from calibration\n",
    "```\n",
    "\n",
    "This block shows how to apply a calibration. This `calibration:` block is read by the `static_spectra.py` script and used for calibration. If it doesn't exist, `static_spectra.py` tries to read the information of the data file. Take a look at `static_spectra.py` within you find the following block:\n",
    "\n",
    "``` python\n",
    "    calibration_config = config.get('calibration', {})\n",
    "    calibration = pysfg.Calibration(\n",
    "        calibration_config.get('central_wl', intensity_data['central_wl']),\n",
    "        calibration_config.get('vis_wl', intensity_data['vis_wl']),\n",
    "        calibration_config.get('calib_central_wl', intensity_data['calib_central_wl']),\n",
    "        calibration_config.get('calib_coeff', intensity_data['calib_coeff'])\n",
    "    )\n",
    "```\n",
    "This block is mostly responsible for the above begaviour. `config` is the python respresentation of the complete `simple_gold.yaml` file. Within `config` a `calibration` key is searched and if not found, and empty dict is passed. Then within the `calibration` dict, `central_wl`, `vis_wl`, `calib_central_wl` and `calib_coeff` is used and if not found the infomration is read of the `intensity_data` dict, that represents the content of the `intensity_data` file, including the header.\n",
    "\n",
    "This calibration is then followed by a `data:` block. This block is mandatory and it consists out of an arbitraty amount of `-` seperated blocks. Each of these blocks is configuring a single spectrum by referencing it to its respective `intensity_data:`, `baseline_data:` and `norm_data:`. However only the `intensity_data:` and `name:` is mandatory. The rest is optional.\n",
    "   \n",
    "``` yaml\n",
    "data:\n",
    "  -\n",
    "    # Path to a data file that contains raw intensity data.\n",
    "    intensity_data:  \"../tests/data/gold.dat\"\n",
    "    # Configure selection of data. E.g. pixels, pp_delays, spectra or scans.\n",
    "    intensity_selector:\n",
    "      # The index of the spectrum that should be used.\n",
    "      spectra: 1\n",
    "    # Path to a data file that contains raw background data.\n",
    "    background_data: \"../tests/data/gold_bg.dat\" # Relative to fpath\n",
    "    background_selector:\n",
    "      spectra: 1\n",
    "      #pixel: Has no function here, as it is Overwritten by the intensity_selector pixels.\n",
    "    name: \"cache/gold_1.json\"\n",
    "```\n",
    "\n",
    "The key `intensity_data` is mapped to a filepath with a *victor* or *vivian* `.dat` data file and the path is relative to the position of this notebook you are looking at right now. The `intensity_selector` is optional and can have up to four sub keys: `pp_delays`, `scans`, `spectra` and `pixel`. For static spectra `spectra` and `pixel` are the most important.  As they are used to trim down the data into a relevant section. `spectra: 1` means take only spectrum 1. Internally a `pysfg.SelectorPP` object is created from this block. The `background_data`  and `background_selector` then works identical but is taking care of the background intensity. `name` is the path to a `.json` file, where the strucutred result of the `static_spectra.py` script is saved as a `json` file. This file can then be used for visualizing and further processing of the spectrum. For educational purpose the full data block is:\n",
    "\n",
    "\n",
    "```yaml\n",
    "data:\n",
    "  -\n",
    "    # Path to a data file that contains raw intensity data.\n",
    "    intensity_data:  \"../tests/data/gold.dat\"\n",
    "    # Configure selection of data. E.g. pixels, pp_delays, spectra or scans.\n",
    "    intensity_selector:\n",
    "      # The index of the spectrum that should be used.\n",
    "      spectra: 1\n",
    "    # Path to a data file that contains raw background data.\n",
    "    background_data: \"../tests/data/gold_bg.dat\" # Relative to fpath\n",
    "    background_selector:\n",
    "      spectra: 1\n",
    "      #pixel: Has no function here, as it is Overwritten by the intensity_selector pixels.\n",
    "    name: \"cache/gold_1.json\"\n",
    "  # You can have as many of these blocks as you want\n",
    "  -\n",
    "    intensity_data:  \"../tests/data/gold.dat\"\n",
    "    intensity_selector:\n",
    "      spectra: 0 # Lets use some other spectrum of the same data file\n",
    "      pixel: [520, 810] # A list in [start, stop, step] order\n",
    "      scans: [0, 3] # use onlay the first three scans.\n",
    "      pp_delays: null # This is the same as not putting it at all\n",
    "    background_data: \"../tests/data/gold_bg.dat\"\n",
    "    background_selector:\n",
    "      spectra: 0\n",
    "    name: \"cache/gold_2.json\"\n",
    "\n",
    "```\n",
    "Where a second `-` block is added. It shows some more possible options and how this script can be used to process arbitrary amount of spectra. Just append another `-` block and configure it to your liking. \n",
    "\n",
    "Now lets see how to actually run this. Execute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run static_spectra.py simple_gold.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log above gives you some information on what is happending. It tels you the path of the importet data, the calibration configuartion and the path where the result is exportet.\n",
    "\n",
    "Next step is to visualize the result. The following is a possible example of what you could do now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the json files and create a pysfg.Spectrum object from them in one go.\n",
    "norm = pysfg.spectrum.json_to_spectrum('cache/gold_1.json')\n",
    "norm2 = pysfg.spectrum.json_to_spectrum('cache/gold_2.json')\n",
    "\n",
    "# Select some pixels for visualization\n",
    "pixel=[460, 810]\n",
    "# Figures\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(norm.wavenumber[slice(*pixel)], norm.basesubed[slice(*pixel)])\n",
    "plt.xlabel('Frequency/cm$^{-1}$')\n",
    "plt.ylabel('Counts')\n",
    "plt.title('Normalization')\n",
    "\n",
    "# Figures\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(norm2.wavenumber, norm2.basesubed)\n",
    "plt.xlabel('Frequency/cm$^{-1}$')\n",
    "plt.ylabel('Counts')\n",
    "plt.title('Normalization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, the calibration is manually passed within the `simple_gold.yaml` file in the `calibration:` section. This is needed, because the read file `../tests/data/gold.dat` contains no correct calibration data. Just open the file and check out the `# calib Coeff=0` line. This is not a correct calibration. The file `../tests/data/sc_quartz.dat` contains a correct calibration. Checkout the file header. It reads: `# calib Coeff=670\t642.101\t0.034274\t0\t0\t0\t0` \n",
    "\n",
    "`simple_quartz.yaml` shows you a configuration that reads the information of the file itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run static_spectra.py simple_quartz.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again import and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = pysfg.spectrum.json_to_spectrum('cache/quartz_1.json')\n",
    "pixel=slice(None)\n",
    "\n",
    "# Figures\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(norm.wavenumber[pixel], norm.basesubed[pixel])\n",
    "plt.xlabel('Frequency/cm$^{-1}$')\n",
    "plt.ylabel('Counts')\n",
    "plt.title('Normalization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I highly encourage you to use this import/export sceme as you see here. Initially it is some overhead, but in time you will realize, that it seperates logic (analysis) from plotting (visualizing) and it will make your live way easier if you start preparing presentations, or papers or what ever, you want to be ablte to quickly change the visualization but not the logic. This sheme allows you to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize a Static Spectrum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To normalize some static spectra, you can use the same template script `static_spectra.py`. However you have to change the specific configuration of course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run static_spectra.py simple_spectrum.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now `simple_spectrum.yaml` contains:\n",
    "    \n",
    "```yaml\n",
    "data:\n",
    "  -\n",
    "    intensity_data:  \"../tests/data/sc_d2o-dopc.dat\"\n",
    "    intensity_selector:\n",
    "      spectra: 1\n",
    "      # you can pass all kwargs of pysfg.SelectorPP here. Thus also:\n",
    "      pixel: [200, 1200]\n",
    "      # scans: [start, stop, step]\n",
    "      # pp_delays: [start, stop, step]\n",
    "    background_data: \"../tests/data/bg_d2o-docpe.dat\"\n",
    "    background_selector:\n",
    "      spectra: 1\n",
    "    # Norm data must be the result of a prior static_spectra.py run.\n",
    "    norm_data: \"./cache/quartz_1.json\"\n",
    "    name: \"cache/sc_d2o-dopc_static.json\"\n",
    "```\n",
    "\n",
    "Basically everything is indentical to the quartz or gold case as we have seen above, but note the `norm_data` line. Here we give it a path to one of the previously exported `quartz_1.json` files, as we want to use this for normalization. There is no `norm_selector` keyword available, if you want to change the normalization, go to the `simple_quarty.yaml` file, add or change one of the blocks in `data:` and reference the result here.\n",
    "\n",
    "Now lets visualize the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pysfg.spectrum.json_to_spectrum('cache/sc_d2o-dopc_static.json')\n",
    "\n",
    "# Figures\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(sc.wavenumber, sc.basesubed)\n",
    "plt.xlabel('Frequency/cm$^{-1}$')\n",
    "plt.ylabel('Counts')\n",
    "plt.title('Normalization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or if you want to plot the normalized version you do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plt.plot(sc.wavenumber, sc.normalized)\n",
    "plt.xlabel('Frequency/cm$^{-1}$')\n",
    "plt.ylabel('A.u.')\n",
    "plt.title('Normalization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you can even take a look at the raw data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plt.plot(sc.pixel, sc.intensity)\n",
    "plt.xlabel('Pixel')\n",
    "plt.ylabel('Counts')\n",
    "plt.title('Normalization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the pixels start at 200 and not at 0, as we have configured it that way in `simple_spectrum.yaml` by passing it `pixel: [200, 1200]`. If you want, you can also create a pandas Dataframe from the result of `simple_spectrum.py` Just do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_json('./cache/sc_d2o-dopc_static.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tio2",
   "language": "python",
   "name": "tio2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
